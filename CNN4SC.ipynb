{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN4SC.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1Jlm03vns1rDOgsnf4zCUv9QgPgwSxlJb","authorship_tag":"ABX9TyMul0mNy9up7B3EXrdRPQTD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"4IJVIX-04QpU"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","\n","class Net(nn.Module):\n","    def __init__(self, lookup_table,kernel_size, num_filt, num_classes, drop_prob, mode='static'):\n","        super(Net, self).__init__()\n","        np.random.seed(99)\n","        self.mode = mode\n","        if mode == 'rand':\n","            ch_num = 1\n","            new_lookup_table = np.random.uniform(-1,1,lookup_table.shape).astype(np.float32)\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(new_lookup_table), freeze=True)\n","        elif mode == 'static':\n","            ch_num = 1\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","        elif mode == 'non-static':\n","            ch_num = 1\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=False)\n","        elif mode == 'multi-channel':\n","            ch_num = 2\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","            self.embedding_mult = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=False)\n","        self.convs = nn.ModuleList([nn.Conv2d(ch_num,num_filt,(size,300),1) for size in kernel_size])\n","        torch.manual_seed(99)\n","        for conv in self.convs:\n","            torch.nn.init.kaiming_uniform_(conv.weight)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","        self.fc = nn.Linear(len(kernel_size)*num_filt, num_classes)\n","        torch.nn.init.uniform_(self.fc.weight)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        embed_out = self.embedding(x).unsqueeze(1)\n","        if self.mode == 'multi-channel':\n","            embed_mult = self.embedding_mult(x).unsqueeze(1)\n","            embed_out = torch.cat((embed_out, embed_mult), 1)\n","        conv_out = [torch.max(self.relu(conv(embed_out)), dim=2)[0].squeeze(-1) for conv in self.convs]\n","        flatten = torch.cat(conv_out, dim=1)\n","        dropouts = self.dropout(flatten)\n","        out = self.fc(dropouts)\n","        return self.softmax(out)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oDXes0f4Tvc"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import time\n","import nltk\n","import os\n","import pandas as pd\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","from sklearn.model_selection import KFold\n","import copy\n","import re\n","import pickle\n","\n","w2v = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin', binary=True)\n","\n","def strCleanup(string):\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" \\( \", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    return string.strip().lower()\n","\n","def makeIdx(data, w2i):\n","    def get_idx(str):\n","        return w2i[str]\n","    for idx_sent, sent in enumerate(data):\n","        for idx, word in enumerate(sent):\n","            data[idx_sent][idx] = get_idx(word)\n","#data is tokenized sentences list\n","def make_w2v(data):\n","    embedding_dic = {}\n","    wv_0 = Word2Vec(data, size=300, window=5, min_count=1)\n","    embedding_arr = np.zeros((len(wv_0.wv.vocab) + 2, 300), dtype=np.float32)\n","    for idx, (word, vec) in enumerate(zip(wv_0.wv.vocab, wv_0.wv.vectors)):\n","        coefs = np.asarray(vec, dtype='float32')\n","        embedding_arr[idx + 2, :] = (coefs / np.linalg.norm(coefs))\n","        embedding_dic[word] = idx + 2\n","    return embedding_dic, embedding_arr\n","#data is set of words\n","def get_w2v(data):\n","    global w2v\n","    tmp_dic = {}\n","    embedding_dic = {}\n","    noword = 0\n","    for word, vec in zip(w2v.vocab, w2v.vectors):\n","        if word in data:\n","            tmp_dic[word] = vec\n","    for word in data:\n","        if tmp_dic.get(word) is None:\n","            tmp_dic[word] = np.random.uniform(-1, 1, size=300)\n","            noword += 1\n","    embedding_arr = np.zeros((len(tmp_dic) + 2, 300), dtype=np.float32)\n","    print(\"PRE words: \", len(data) - noword)\n","    rand = np.random.uniform(-1, 1, size=300)\n","    embedding_arr[0] = rand / np.linalg.norm(rand)\n","    rand = np.random.uniform(-1, 1, size=300)\n","    embedding_arr[1] = rand / np.linalg.norm(rand)\n","    for idx, (word, vec) in enumerate(tmp_dic.items()):\n","        coefs = np.asarray(vec, dtype='float32')\n","        embedding_arr[idx + 2, :] = (coefs / np.linalg.norm(coefs))\n","        embedding_dic[word] = idx + 2\n","    return embedding_dic, embedding_arr\n","\n","def MR_Preprocess(embed_kinds = 0):\n","    global w2v\n","    f_pos = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.pos\", 'r', encoding='latin-1')\n","    f_neg = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.neg\", 'r', encoding='latin-1')\n","    lines_pos = f_pos.readlines()\n","    lines_neg = f_neg.readlines()\n","    tokenized_pos = [strCleanup(line).split() for line in lines_pos]\n","    tokenized_neg = [strCleanup(line).split() for line in lines_neg]\n","    pos_label = [0] * len(tokenized_pos)\n","    neg_label = [1] * len(tokenized_neg)\n","    f_pos.close()\n","    f_neg.close()\n","    tokenized = tokenized_pos+tokenized_neg\n","    label = pos_label+neg_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    noword = 0\n","\n","    if not os.path.isfile(\"/content/drive/MyDrive/Data/rt-polaritydata/MR_dic.pickle\"):\n","        if embed_kinds is 0:\n","            wv_0 = Word2Vec(tokenized, size=300, window=5, min_count=1)\n","            embedding_arr = np.zeros((len(wv_0.wv.vocab)+2, 300), dtype=np.float32)\n","            for idx, (word,vec) in enumerate(zip(wv_0.wv.vocab, wv_0.wv.vectors)):\n","                coefs = np.asarray(vec, dtype='float32')\n","                embedding_arr[idx+2,:] = (coefs/np.linalg.norm(coefs))\n","                embedding_dic[word] = idx+2\n","        elif embed_kinds is 1:\n","            tmp_dic = {}\n","            for word,vec in zip(w2v.vocab, w2v.vectors):\n","                if word in tokenized_set:\n","                    tmp_dic[word] = vec\n","            for word in tokenized_set:\n","                if tmp_dic.get(word) is None:\n","                    tmp_dic[word] = np.random.uniform(-1, 1, size=300)\n","                    noword +=1\n","            embedding_arr = np.zeros((len(tmp_dic)+2, 300), dtype=np.float32)\n","            print(\"PRE words: \",len(tokenized_set) - noword)\n","            rand = np.random.uniform(-1,1,size = 300)\n","            embedding_arr[0] = rand/np.linalg.norm(rand)\n","            rand = np.random.uniform(-1,1,size = 300)\n","            embedding_arr[1] = rand/np.linalg.norm(rand)\n","            for idx, (word, vec) in enumerate(tmp_dic.items()):\n","                coefs = np.asarray(vec, dtype='float32')\n","                embedding_arr[idx+2,:] = (coefs/np.linalg.norm(coefs))\n","                embedding_dic[word] = idx+2\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_dic.pickle', 'wb') as fw:\n","            pickle.dump(embedding_dic,fw)\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_arr.pickle', 'wb') as fa:\n","            pickle.dump(embedding_arr,fa)\n","    else:\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_dic.pickle', 'rb') as fw:\n","            embedding_dic = pickle.load(fw)\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_arr.pickle', 'rb') as fa:\n","            embedding_arr = pickle.load(fa)\n","    print(embedding_dic)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    tokenized_df = [[tokenized[i], label[i]] for i in range(len(tokenized))]\n","    print(embedding_arr.shape)\n","    return tokenized_df, tokenized_set, embedding_arr\n","\n","def STT_Preprocess(embed_kinds = 0):\n","    if not os.path.isfile(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/SST_Merged.csv\"):\n","        sentences = pd.read_csv(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\\t\")\n","        dic = pd.read_csv(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/dictionary.txt\", sep=\"|\", names=['phrase', 'phrase ids'])\n","        labels = pd.read_csv(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/sentiment_labels.txt\", sep=\"|\")\n","        split = pd.read_csv(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/datasetSplit.txt\")\n","        sentence_phrase = pd.merge(sentences, dic, left_on='sentence', right_on='phrase')\n","        train_test_split = pd.merge(sentence_phrase, split, on='sentence_index')\n","        merged = pd.merge(train_test_split, labels, on='phrase ids')\n","        merged.to_csv(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/SST_Merged.csv\")\n","    else:\n","        merged = pd.read_csv(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/SST_Merged.csv\")\n","\n","    def tokenize(str):\n","        return strCleanup(str).split()\n","    def check_vocabsize(arr):\n","        return len(arr)\n","\n","    merged['sentence'] = merged['sentence'].transform(tokenize)\n","    vocab_list = []\n","    for data in merged['sentence']:\n","        vocab_list.extend(data)\n","    vocab_list = list(set(vocab_list))\n","    print('vocab size: ', len(vocab_list))\n","    def make_label(score):\n","        if 0 <= score <= 0.2:\n","            return 0\n","        elif 0.2 < score <= 0.4:\n","            return 1\n","        elif 0.4 < score <= 0.6:\n","            return 2\n","        elif 0.6 < score <= 0.8:\n","            return 3\n","        elif 0.8 < score <= 1:\n","            return 4\n","    merged['label'] = merged['sentiment values'].transform(make_label)\n","    if not os.path.isfile(\"/content/drive/MyDrive/Data/stanfordSentimentTreebank/STT_arr.pickle\"):\n","        if embed_kinds is 0:\n","            embedding_dic, embedding_arr = make_w2v(merged['sentence'].to_numpy())\n","        elif embed_kinds is 1:\n","            embedding_dic, embedding_arr = get_w2v(vocab_list)\n","        with open('/content/drive/MyDrive/Data/stanfordSentimentTreebank/STT_dic.pickle', 'wb') as fw:\n","            pickle.dump(embedding_dic,fw)\n","        with open('/content/drive/MyDrive/Data/stanfordSentimentTreebank/STT_arr.pickle', 'wb') as fa:\n","            pickle.dump(embedding_arr,fa)\n","    else:\n","        with open('/content/drive/MyDrive/Data/stanfordSentimentTreebank/STT_dic.pickle', 'rb') as fw:\n","            embedding_dic = pickle.load(fw)\n","        with open('/content/drive/MyDrive/Data/stanfordSentimentTreebank/STT_arr.pickle', 'rb') as fa:\n","            embedding_arr = pickle.load(fa)\n","    makeIdx(merged['sentence'], embedding_dic)\n","    train = merged[merged['splitset_label'] == 1]\n","    test = merged[merged['splitset_label'] == 2]\n","    dev = merged[merged['splitset_label'] == 3]\n","    train_df = [[row['sentence'], row['label']] for index,row in train.iterrows()]\n","    test_df = [[row['sentence'], row['label']] for index,row in test.iterrows()]\n","    dev_df = [[row['sentence'], row['label']] for index,row in dev.iterrows()]\n","\n","    return train_df, test_df, dev_df, vocab_list, embedding_arr\n","\n","def SUBJ_Preprocess(embed_kinds = 0):\n","    global w2v\n","    f_sub = open(\"/content/drive/MyDrive/Data/rotten_imdb.tar/quote.tok.gt9.5000\", 'r', encoding='latin-1')\n","    f_obj = open(\"/content/drive/MyDrive/Data/rotten_imdb.tar/plot.tok.gt9.5000\", 'r', encoding='latin-1')\n","    lines_sub = f_sub.readlines()\n","    lines_obj = f_obj.readlines()\n","    tokenized_sub = [strCleanup(line).split() for line in lines_sub]\n","    tokenized_obj = [strCleanup(line).split() for line in lines_obj]\n","    sub_label = [0] * len(tokenized_sub)\n","    obj_label = [1] * len(tokenized_obj)\n","    f_sub.close()\n","    f_obj.close()\n","    tokenized = tokenized_sub+tokenized_obj\n","    label = sub_label+obj_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    if not os.path.isfile(\"/content/drive/MyDrive/Data/TREC/TREC_arr.pickle\"):\n","        if embed_kinds is 0:\n","            embedding_dic, embedding_arr = make_w2v(tokenized)\n","        elif embed_kinds is 1:\n","            embedding_dic, embedding_arr = get_w2v(tokenized_set)\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_dic.pickle', 'wb') as fw:\n","            pickle.dump(embedding_dic,fw)\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_arr.pickle', 'wb') as fa:\n","            pickle.dump(embedding_arr,fa)\n","    else:\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_dic.pickle', 'rb') as fw:\n","            embedding_dic = pickle.load(fw)\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_arr.pickle', 'rb') as fa:\n","            embedding_arr = pickle.load(fa)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    tokenized_df = [[tokenized[i], label[i]] for i in range(len(tokenized))]\n","    print(embedding_arr.shape)\n","    return tokenized_df, tokenized_set, embedding_arr\n","\n","def TREC_Preprocess(embed_kinds = 0):\n","    f_train = open(\"/content/drive/MyDrive/Data/TREC/train_5500.label.txt\", 'r', encoding='latin-1')\n","    f_test = open(\"/content/drive/MyDrive/Data/TREC/TREC_10.label.txt\", 'r', encoding='latin-1')\n","    lines_train = f_train.readlines()\n","    lines_test = f_test.readlines()\n","    train_label = []\n","    test_label = []\n","    train_tokenized = []\n","    test_tokenized = []\n","\n","    for line in lines_train:\n","        line = line.split(\":\")\n","        train_label.append(line[0])\n","        # ? is in every sentence so delete\n","        line = strCleanup(line[1]).split()[1:-1]\n","        train_tokenized.append(line)\n","    for line in lines_test:\n","        line = line.split(\":\")\n","        test_label.append(line[0])\n","        # ? is in every sentence so delete\n","        line = strCleanup(line[1]).split()[1:-1]\n","        test_tokenized.append(line)\n","    tokenized = train_tokenized+test_tokenized\n","    f_train.close()\n","    f_test.close()\n","    def make_label(cat):\n","        if cat == 'ABBR':\n","            return 0\n","        elif cat == 'ENTY':\n","            return 1\n","        elif cat == 'DESC':\n","            return 2\n","        elif cat == 'HUM':\n","            return 3\n","        elif cat == 'LOC':\n","            return 4\n","        elif cat == 'NUM':\n","            return 5\n","    label = np.array(train_label+test_label)\n","    lebelling = np.vectorize(make_label)\n","    label = lebelling(label)\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    if not os.path.isfile(\"/content/drive/MyDrive/Data/TREC/TREC_arr.pickle\"):\n","        if embed_kinds is 0:\n","            embedding_dic, embedding_arr = make_w2v(tokenized)\n","        elif embed_kinds is 1:\n","            embedding_dic, embedding_arr = get_w2v(tokenized_set)\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_dic.pickle', 'wb') as fw:\n","            pickle.dump(embedding_dic,fw)\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_arr.pickle', 'wb') as fa:\n","            pickle.dump(embedding_arr,fa)\n","    else:\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_dic.pickle', 'rb') as fw:\n","            embedding_dic = pickle.load(fw)\n","        with open('/content/drive/MyDrive/Data/TREC/TREC_arr.pickle', 'rb') as fa:\n","            embedding_arr = pickle.load(fa)\n","    makeIdx(tokenized, embedding_dic)\n","    train_df = []\n","    test_df = []\n","    for idx in range(len(tokenized)):\n","        if idx < len(train_tokenized):\n","            train_df.append([tokenized[idx], label[idx]])\n","        else:\n","            test_df.append([tokenized[idx], label[idx]])\n","    return train_df, test_df, tokenized_set, embedding_arr\n","\n","def CRnMPQA_Preprocess(pos_path, neg_path, embed_kinds = 0, ):\n","    global w2v\n","    f_pos = open(pos_path, 'r', encoding='latin-1')\n","    f_neg = open(neg_path, 'r', encoding='latin-1')\n","    lines_pos = f_pos.readlines()\n","    lines_neg = f_neg.readlines()\n","    tokenized_pos = [strCleanup(line).split() for line in lines_pos]\n","    tokenized_neg = [strCleanup(line).split() for line in lines_neg]\n","    pos_label = [0] * len(tokenized_pos)\n","    neg_label = [1] * len(tokenized_neg)\n","    f_pos.close()\n","    f_neg.close()\n","    tokenized = tokenized_pos+tokenized_neg\n","    label = pos_label+neg_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    if embed_kinds == 0:\n","        embedding_dic, embedding_arr = make_w2v(tokenized)\n","    elif embed_kinds == 1:\n","        embedding_dic, embedding_arr = get_w2v(tokenized_set)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    tokenized_df = [[tokenized[i], label[i]] for i in range(len(tokenized))]\n","    print(embedding_arr.shape)\n","    return tokenized_df, tokenized_set, embedding_arr\n","\n","def batchPad(batch):\n","    max_len = max([len(sent[0]) for sent in batch])\n","    if max_len <5:\n","        max_len = 5\n","    rows = []\n","    batch_labels = []\n","    for i in range(len(batch)):\n","        rows.append(np.pad(np.array(batch[i][0][:]), (0, max_len), 'constant', constant_values=1)[:max_len])\n","        batch_labels.append(batch[i][1])\n","        #print(batch_labels)\n","    return torch.LongTensor(np.concatenate(rows, axis=0).reshape(-1, max_len)), torch.LongTensor(batch_labels)\n","\n","\n","#MR_data, MR_wordset, lookup_table = MR_Preprocess(1)\n","#MR_Train = MR_data[:5000]+MR_data[5332:10333]\n","#MR_Test = MR_data[5000:5332]+MR_data[10333:]\n","#np.random.shuffle(MR_Test)\n","#STT_traindata, STT_testdata, STT_devdata, STT_wordset, lookup_table = STT_Preprocess(1)\n","SUBJ_data, SUBJ_wordset, lookup_table = SUBJ_Preprocess(1)\n","SUBJ_Train = SUBJ_data[:4700]+ SUBJ_data[5000:9700]\n","SUBJ_Test = SUBJ_data[4700:5000]+SUBJ+data[9700:]\n","#TREC_traindata, TREC_testdata, TREC_wordset, lookup_table = TREC_Preprocess(1)\n","#CR_data, CR_wordset, lookup_table = CRnMPQA_Preprocess(\"/content/drive/MyDrive/Data/CR/custrev.pos.txt\", \"/content/drive/MyDrive/Data/CR/custrev.neg.txt\", 1)\n","#MPQA_data, MPQA_wordset, lookup_table = CRnMPQA_Preprocess(\"/content/drive/MyDrive/Data/MPQA/mpqa.pos.txt\", \"/content/drive/MyDrive/Data/MPQA/mpqa.neg.txt\", 1)\n","\n","\n","# Set Model for MR Data\n","net = Net(lookup_table, [3,4,5], 100, 5, 0.5, mode='multi-channel')\n","\n","# HyperParameters\n","dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('current device: ', dev)\n","net.to(dev)\n","k_folds = 10\n","num_epochs = 100\n","batch_size = 50\n","loss_func = nn.CrossEntropyLoss()\n","parameters = filter(lambda p: p.requires_grad, net.parameters())\n","optimizer = optim.Adadelta(parameters, lr = 0.1)\n","\n","torch.manual_seed(99)\n","running_loss = 0\n","correct = 0\n","total = 0\n","\n","fold_results = {}\n","is_kfold = False\n","if is_kfold:\n","    kfold = KFold(n_splits=k_folds, shuffle=True)\n","    for fold, (train, test) in enumerate(kfold.split(SUBJ_Train)):\n","        train_sampler = torch.utils.data.SubsetRandomSampler(train)\n","        test_sampler = torch.utils.data.SubsetRandomSampler(test)\n","        trainloader = torch.utils.data.DataLoader(SUBJ_Train, batch_size=batch_size, sampler=train_sampler, collate_fn=batchPad)\n","        testloader = torch.utils.data.DataLoader(SUBJ_Train, batch_size=batch_size, sampler=test_sampler, collate_fn=batchPad)\n","        unseenloader = torch.utils.data.DataLoader(SUBJ_Test, batch_size=batch_size, collate_fn=batchPad)\n","\n","        for epoch in range(num_epochs):\n","            running_loss = 0.0\n","            correct = 0\n","            total = 0\n","            for idx,(x, y) in enumerate(iter(trainloader), 0):\n","                net.train()\n","                x, y = x.to(dev), y.to(dev)\n","\n","                out = net(x)\n","                loss = loss_func(out, y)\n","                loss.backward()\n","                nn.utils.clip_grad_norm_(parameters, max_norm=3)\n","                optimizer.step()\n","                optimizer.zero_grad()\n","\n","                running_loss += loss.item()\n","                if idx % 50 == 49:\n","                    correct = (torch.max(out, 1)[1] == y).sum().item()\n","                    total = batch_size\n","                    print('Training Accuracy: %d ' %(100.0 * correct / total))\n","                    print('%d/%d' % (correct, total))\n","                    print('RunningLoss %5d: %.3f' %(idx+1, running_loss))\n","\n","        correct = 0\n","        total = 0\n","        net.eval()\n","        \"\"\"\n","        with torch.no_grad():\n","            for (x, y)  in iter(testloader):\n","                x, y = x.to(dev), y.to(dev)\n","                out = net.forward(x)\n","                predicted = torch.max(out, 1)[1]\n","                loss = loss_func(out, y)\n","                print(\"test loss: \", loss.item())\n","\n","                correct += (predicted == y).sum().item()\n","                total += len(y)\n","                print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n","                print('--------------------------------')\n","                fold_results[fold] = 100.0 * (correct / total)\n","        \"\"\"\n","        with torch.no_grad():\n","            for (x, y)  in iter(unseenloader):\n","                x, y = x.to(dev), y.to(dev)\n","                out = net.forward(x)\n","                predicted = torch.max(out, 1)[1]\n","                loss = loss_func(out, y)\n","                print(\"test loss: \", loss.item())\n","\n","                correct += (predicted == y).sum().item()\n","                total += len(y)\n","                print('Accuracy for fold %d: %.3f %%' % (fold, 100.0 * correct / total))\n","                print('--------------------------------')\n","                fold_results[fold] = 100.0 * (correct / total)\n","\n","    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {k_folds} FOLDS')\n","    print('--------------------------------')\n","    sum = 0.0\n","    for key, value in fold_results.items():\n","        print(f'Fold {key}: {value} %')\n","        sum += value\n","        print(f'Average: {sum/len(fold_results.items())} %')\n","else:\n","    trainloader = torch.utils.data.DataLoader(STT_traindata, batch_size=batch_size, shuffle=True, collate_fn=batchPad)\n","    testloader = torch.utils.data.DataLoader(STT_testdata, batch_size=batch_size, shuffle= True, collate_fn=batchPad)\n","\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","        for idx, (x, y) in enumerate(iter(trainloader), 0):\n","            net.train()\n","            x, y = x.to(dev), y.to(dev)\n","\n","            out = net(x)\n","            loss = loss_func(out, y)\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(parameters, max_norm=3)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            running_loss += loss.item()\n","            if idx % 50 == 49:\n","                correct = (torch.max(out, 1)[1] == y).sum().item()\n","                total = batch_size\n","                print('Training Accuracy: %d ' % (100.0 * correct / total))\n","                print('%d/%d' % (correct, total))\n","                print('RunningLoss %5d: %.3f' % (idx + 1, running_loss))\n","\n","    correct_eachbatch = 0\n","    total_eachbatch = 0\n","    correct = 0\n","    total = 0\n","    net.eval()\n","    with torch.no_grad():\n","        for (x, y) in iter(testloader):\n","            x, y = x.to(dev), y.to(dev)\n","            out = net.forward(x)\n","            predicted = torch.max(out, 1)[1]\n","            loss = loss_func(out, y)\n","            print(\"test loss: \", loss.item())\n","            correct_eachbatch += (predicted == y).sum().item()\n","            correct += correct_eachbatch\n","            total_eachbatch += len(y)\n","            total += total_eachbatch\n","            correct_eachbatch = 0\n","            total_eachbatch = 0\n","            print('Accuracy for test_batch: %.3f %%' % (100.0 * correct / total))\n","            print('--------------------------------')\n","\n","print('Accuracy for test: %.3f %%' % (100.0 * correct / total))\n","print('--------------------------------')"],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"word2vec.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1lc9L18xE_sHOZpHgJzoWgQVGeOsst5BI","authorship_tag":"ABX9TyNm+4FaeinDpxKhGqJSUeD7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gUKVZPzKhQnu"},"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","\n","class W2V(nn.Module):\n","    def __init__(self, word_size, projection_size, drop_prob):\n","        super(W2V, self).__init__()\n","        np.random.seed(99)\n","        self.fc_one = nn.Linear(word_size, projection_size)\n","        #self.dropout = nn.Dropout(p=drop_prob)\n","        self.relu = nn.ReLU()\n","        self.fc_two = nn.Linear(projection_size, word_size)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        out = self.fc_one(x)\n","        #out = self.dropout(out)\n","        out = self.relu(out)\n","        out = self.fc_two(out)\n","        return self.softmax(out)\n","    \n","    def get_vector(self, x):\n","        out = self.fc_one(x)\n","        return out\n","\n","class Char2WV(nn.Module):\n","    def __init__(self, word_size, vocab_size, projection_size, drop_prob):\n","        super(Char2WV, self).__init__()\n","        np.random.seed(99)\n","        self.fc_one = nn.Linear(word_size, projection_size)\n","        #self.dropout = nn.Dropout(p=drop_prob)\n","        self.relu = nn.ReLU()\n","        self.fc_two = nn.Linear(projection_size, projection_size)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        out = self.fc_one(x)\n","        #out = self.dropout(out)\n","        out = self.relu(out)\n","        out = self.fc_two(out)\n","        return self.softmax(out)\n","    \n","    def get_vector(self, x):\n","        out = self.fc_one(x)\n","        return out\n","\n","class Net(nn.Module):\n","    def __init__(self, lookup_table, lookup_char,kernel_size, num_filt, num_classes, drop_prob, mode='static'):\n","        super(Net, self).__init__()\n","        np.random.seed(99)\n","        self.mode = mode\n","        if mode == 'rand':\n","            ch_num = 1\n","            new_lookup_table = np.random.uniform(-1,1,lookup_table.shape).astype(np.float32)\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(new_lookup_table), freeze=True)\n","        elif mode == 'static':\n","            ch_num = 1\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","        elif mode == 'non-static':\n","            ch_num = 1\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=False)\n","        elif mode == 'multi-channel':\n","            ch_num = 2\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","            self.embedding_mult = nn.Embedding.from_pretrained(torch.from_numpy(lookup_char), freeze=True)\n","        self.convs = nn.ModuleList([nn.Conv2d(ch_num,num_filt,(size,100),1) for size in kernel_size])\n","        torch.manual_seed(99)\n","        for conv in self.convs:\n","            torch.nn.init.kaiming_uniform_(conv.weight)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","        self.fc = nn.Linear(len(kernel_size)*num_filt, num_classes)\n","        torch.nn.init.uniform_(self.fc.weight)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        embed_out = self.embedding(x).unsqueeze(1)\n","        if self.mode == 'multi-channel':\n","            embed_mult = self.embedding_mult(x).unsqueeze(1)\n","            embed_out = torch.cat((embed_out, embed_mult), 1)\n","        conv_out = [torch.max(self.relu(conv(embed_out)), dim=2)[0].squeeze(-1) for conv in self.convs]\n","        flatten = torch.cat(conv_out, dim=1)\n","        dropouts = self.dropout(flatten)\n","        out = self.fc(dropouts)\n","        return self.softmax(out)\n","\n","class Net_Deep(nn.Module):\n","    def __init__(self, lookup_table, lookup_char,kernel_size, num_filt, num_classes, drop_prob, mode='static'):\n","        super(Net_Deep, self).__init__()\n","        np.random.seed(99)\n","        self.mode = mode\n","        if mode == 'rand':\n","            ch_num = 1\n","            new_lookup_table = np.random.uniform(-1,1,lookup_table.shape).astype(np.float32)\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(new_lookup_table), freeze=True)\n","        elif mode == 'static':\n","            ch_num = 1\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","        elif mode == 'non-static':\n","            ch_num = 1\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=False)\n","        elif mode == 'multi-channel':\n","            ch_num = 2\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","            self.embedding_mult = nn.Embedding.from_pretrained(torch.from_numpy(lookup_char), freeze=True)\n","        self.convs = nn.ModuleList([nn.Conv2d(ch_num,num_filt,(size,1),1) for size in kernel_size])\n","        self.maxpool = nn.MaxPool2d((2,1))\n","        self.convs2 = nn.ModuleList([nn.Conv2d(num_filt,num_filt,(size,1),1) for size in kernel_size])\n","        self.convs3 = nn.ModuleList([nn.Conv2d(num_filt,num_filt,(size,1),1) for size in kernel_size])\n","        self.convs4 = nn.ModuleList([nn.Conv2d(num_filt,num_filt,(size,100),1) for size in kernel_size])     \n","        if mode == 'multi-channel':\n","            self.wordchar_conv = nn.Conv2d(2, num_filt, (1,100), 1)\n","            self.wordchar_conv2 = nn.ModuleList([nn.Conv2d(num_filt,num_filt,(size,1),1) for size in kernel_size])\n","        elif mode == 'static':\n","            self.wordchar_conv = nn.Conv2d(1, num_filt, (1,100), 1)\n","            self.wordchar_conv2 = nn.ModuleList([nn.Conv2d(num_filt,num_filt,(size,1),1) for size in kernel_size])\n","        torch.manual_seed(99)\n","        for idx, conv in enumerate(self.convs):\n","            torch.nn.init.kaiming_uniform_(conv.weight)\n","            torch.nn.init.kaiming_uniform_(self.convs2[idx].weight)\n","            torch.nn.init.kaiming_uniform_(self.convs3[idx].weight)\n","            torch.nn.init.kaiming_uniform_(self.convs4[idx].weight)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(p=drop_prob)\n","        self.fc = nn.Linear(len(kernel_size)*num_filt, num_classes)\n","        torch.nn.init.uniform_(self.fc.weight)\n","        self.fc2 = nn.Linear(len(kernel_size)*num_filt, num_classes)\n","        torch.nn.init.uniform_(self.fc2.weight)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        embed_out = self.embedding(x).unsqueeze(1)\n","        if self.mode == 'multi-channel':\n","            embed_mult = self.embedding_mult(x).unsqueeze(1)\n","            embed_out = torch.cat((embed_out, embed_mult), 1)\n","        conv_out = list()\n","        out = self.wordchar_conv(embed_out)\n","        conv_out = [torch.max(self.relu(conv(out)), dim=2)[0].squeeze(-1) for conv in self.wordchar_conv2]\n","        #for conv in self.convs:\n","            #conv_out.append(self.maxpool(self.relu(conv(embed_out)).permute(0,3,2,1)))\n","        #    conv_out.append(self.relu(conv(embed_out)).permute(0,3,2,1))\n","        #conv2_out = list()\n","        #for idx,conv in enumerate(self.convs2):\n","            #conv2_out.append(self.maxpool(self.relu(conv(conv_out[idx])).permute(0,3,2,1)))\n","        #    conv2_out.append(self.relu(conv(conv_out[idx])).permute(0,3,2,1))\n","        #conv3_out = list()\n","        #for idx,conv in enumerate(self.convs3):\n","        #    #conv3_out.append(self.maxpool(self.relu(conv(conv2_out[idx])).permute(0,3,2,1)))\n","        #    conv3_out.append(self.relu(conv(conv2_out[idx])).permute(0,3,2,1))\n","        #conv4_out = [torch.max(self.relu(conv(conv3_out[idx])), dim=2)[0].squeeze(-1) for idx,conv in enumerate(self.convs4)]\n","        flatten = torch.cat(conv_out, dim=1)\n","        dropouts = self.dropout(flatten)\n","        out = self.fc(dropouts)\n","        #out = self.fc2(out)\n","        #out = self.dropout(out)\n","        return self.softmax(out)\n","\n","class Net_1D(nn.Module):\n","    def __init__(self, lookup_table, lookup_char, input_feature, input_length, num_classes, drop_prob, embed='multi-channel'):\n","        super(Net, self).__init__()\n","        np.random.seed(99)\n","        self.embed = embed\n","\n","        if embed == 'static':\n","            ch_num = 1\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","            l6_frame_length = int((input_length - 96)/27)\n","            self.conv = nn.Sequential(\n","                nn.Conv1d(input_feature, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3)\n","            )\n","        elif embed == 'multi-channel':\n","            ch_num = 2\n","            self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(lookup_table), freeze=True)\n","            self.embedding_mult = nn.Embedding.from_pretrained(torch.from_numpy(lookup_char), freeze=True)\n","            l6_frame_length = int((input_length - 96)/27)\n","            self.conv = nn.Sequential(\n","                nn.Conv1d(input_feature*2, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3)\n","            )\n","            self.fc = nn.Sequential(\n","                nn.Linear(l6_frame_length * 256, 1024),\n","                nn.Dropout(p=drop_prob),\n","                nn.Linear(1024, 1024),\n","                nn.Dropout(p=drop_prob),\n","                nn.Linear(1024, num_classes)\n","            )\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        embed_out = self.embedding(x).unsqueeze(1)\n","        if self.mode == 'multi-channel':\n","            embed_mult = self.embedding_mult(x).unsqueeze(1)\n","            embed_out = torch.cat((embed_out, embed_mult), 1)\n","        out = self.conv(x)\n","        out = out.view(len(x), -1)\n","        out = self.fc(out)\n","        return self.softmax(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0B67OYc6dzu"},"source":["!pip install gensim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DdODrI9wldhB"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pandas as pd\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.cluster import KMeansClusterer\n","from sklearn.cluster import KMeans\n","import pickle\n","import re\n","import matplotlib.pyplot as plt\n","import os\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","import time\n","\n","alphabet_dic = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14, 'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25,\n"," '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35,\n"," \"-\":36, ',':37, ';':38, '.':39, '!':40, '?':41, ':':42, '\\'':43, '\"':44, '/':45, '\\\\':46, '|':47, '_':48, '@':49, '#':50, '$':51, '%':52, 'ˆ':53, '&':54, '*':55, '~':56, '`':57, '+':58,'=':59, '<':60, '>':61, '(':62, ')':63, '[':64, ']':65, '{':66, '}':67, '\\n': 68}\n","data_addr = \"/content/drive/MyDrive/Data\"\n","def strCleanup(string):\n","    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n","    string = re.sub(r\"\\'s\", \" \\'s\", string)\n","    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n","    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n","    string = re.sub(r\"\\'re\", \" \\'re\", string)\n","    string = re.sub(r\"\\'d\", \" \\'d\", string)\n","    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n","    string = re.sub(r\",\", \" , \", string)\n","    string = re.sub(r\"!\", \" ! \", string)\n","    string = re.sub(r\"\\(\", \" \\( \", string)\n","    string = re.sub(r\"\\)\", \" \\) \", string)\n","    string = re.sub(r\"\\?\", \" \\? \", string)\n","    string = re.sub(r\"\\s{2,}\", \" \", string)\n","    return string.strip().lower()\n","\n","def makeIdx(data, w2i):\n","    def get_idx(str):\n","        return w2i[str]\n","    for idx_sent, sent in enumerate(data):\n","        for idx, word in enumerate(sent):\n","            data[idx_sent][idx] = get_idx(word)\n","\n","def batch_onehot(line):\n","    global embedding\n","    global vocablen\n","    inoutvector_list = list()\n","    # 0번째, 1번째, -1번째, -2번째 따로, 나머지 그대로\n","    tmp_vector = np.zeros((1,vocablen))\n","    tmp_vector[0][embedding[line[0]]] = 1\n","    inoutvector_list.append((tmp_vector, embedding[line[1]]))\n","    inoutvector_list.append((tmp_vector, embedding[line[2]]))\n","\n","    tmp_vector = np.zeros((1,vocablen))\n","    tmp_vector[0][embedding[line[1]]] = 1\n","    inoutvector_list.append((tmp_vector, embedding[line[0]]))\n","    inoutvector_list.append((tmp_vector, embedding[line[2]]))\n","    inoutvector_list.append((tmp_vector, embedding[line[3]]))\n","\n","    for idx,word in enumerate(line[2:-2]):\n","        tmp_vector = np.zeros((1,vocablen))\n","        tmp_vector[0][embedding[line[idx]]] = 1\n","        inoutvector_list.append((tmp_vector, embedding[line[idx-2]]))\n","\n","        inoutvector_list.append((tmp_vector, embedding[line[idx-1]]))\n","\n","        inoutvector_list.append((tmp_vector, embedding[line[idx+1]]))\n","\n","        inoutvector_list.append((tmp_vector, embedding[line[idx+2]]))\n","    \n","    tmp_vector = np.zeros((1,vocablen))\n","    tmp_vector[0][embedding[line[-2]]] = 1\n","    inoutvector_list.append((tmp_vector, embedding[line[-4]]))\n","    inoutvector_list.append((tmp_vector, embedding[line[-3]]))\n","    inoutvector_list.append((tmp_vector, embedding[line[-1]]))\n","\n","    tmp_vector = np.zeros((1,vocablen))\n","    tmp_vector[0][embedding[line[-1]]] = 1\n","    inoutvector_list.append((tmp_vector, embedding[line[-3]]))\n","    inoutvector_list.append((tmp_vector, embedding[line[-2]]))\n","\n","    return inoutvector_list\n","\n","def batch_onehot_window1(line):\n","    global embedding\n","    global vocablen\n","    invector_list = list()\n","    outvector_list = list()\n","    # 0번째, 1번째, -1번째, -2번째 따로, 나머지 그대로\n","    tmp_vector = np.zeros(vocablen)\n","    tmp_vector[embedding[line[0]]] = 1\n","    invector_list.append(tmp_vector)\n","    outvector_list.append(embedding[line[1]])\n","\n","    for idx,word in enumerate(line[1:-1]):\n","        tmp_vector = np.zeros(vocablen)\n","        tmp_vector[embedding[line[idx]]] = 1\n","\n","        invector_list.append(tmp_vector)\n","        outvector_list.append(embedding[line[idx-1]])\n","\n","        invector_list.append(tmp_vector)\n","        outvector_list.append(embedding[line[idx+1]])\n","\n","    tmp_vector = np.zeros(vocablen)\n","    tmp_vector[embedding[line[-1]]] = 1\n","    invector_list.append(tmp_vector)\n","    outvector_list.append(embedding[line[-2]])\n","    return invector_list, outvector_list\n","\n","#char_to_char\n","\n","def batch_onehot_window1_char(line):\n","    global alphabet_dic\n","    vec_line = list()\n","\n","    char_len = len(alphabet_dic)\n","    invector_list = list()\n","    outvector_list = list()\n","    for idx,word in enumerate(line):\n","        tmp_vector = np.zeros(char_len)\n","        for alphabet in word:\n","            try:\n","                tmp_vector[alphabet_dic[alphabet]] = 1\n","            except KeyError:\n","                continue\n","        vec_line.append(tmp_vector)\n","    \n","    invector_list.append(vec_line[0])\n","    outvector_list.append(vec_line[1])\n","\n","    for idx,word in enumerate(line[1:-1]):\n","        invector_list.append(vec_line[idx])\n","        outvector_list.append(vec_line[idx-1])\n","\n","        invector_list.append(vec_line[idx])\n","        outvector_list.append(vec_line[idx+1])\n","    \n","    invector_list.append(vec_line[-1])\n","    outvector_list.append(vec_line[-2])\n","\n","    return invector_list, outvector_list\n","\n","def batch_onehot_window1_chartoword(line):\n","    global alphabet_dic\n","    global embedding\n","    global vocablen\n","    global w2v\n","    vec_line = list()\n","\n","    char_len = len(alphabet_dic)\n","    invector_list = list()\n","    outvector_list = list()\n","    for idx,word in enumerate(line):\n","        tmp_vector = np.zeros(char_len)\n","        for alphabet in word:\n","            try:\n","                tmp_vector[alphabet_dic[alphabet]] = 1\n","            except KeyError:\n","                continue\n","        vec_line.append(tmp_vector)\n","    \n","    invector_list.append(vec_line[0])\n","    tmp_vector = np.zeros(vocablen)\n","    tmp_vector[embedding[line[1]]] = 1\n","    out_vec = w2v.get_vector(torch.FloatTensor(tmp_vector))\n","    out_vec = np.asarray(out_vec.detach().numpy(), dtype='float32')\n","    outvector_list.append(out_vec)\n","\n","    for idx,word in enumerate(line[1:-1]):\n","        invector_list.append(vec_line[idx])\n","        tmp_vector = np.zeros(vocablen)\n","        tmp_vector[embedding[line[idx-1]]] = 1\n","        out_vec = w2v.get_vector(torch.FloatTensor(tmp_vector))\n","        out_vec = np.asarray(out_vec.detach().numpy(), dtype='float32')\n","        outvector_list.append(out_vec)\n","\n","        invector_list.append(vec_line[idx])\n","        tmp_vector = np.zeros(vocablen)\n","        tmp_vector[embedding[line[idx+1]]] = 1\n","        out_vec = w2v.get_vector(torch.FloatTensor(tmp_vector))\n","        out_vec = np.asarray(out_vec.detach().numpy(), dtype='float32')\n","        outvector_list.append(out_vec)\n","    \n","    invector_list.append(vec_line[-1])\n","    tmp_vector = np.zeros(vocablen)\n","    tmp_vector[embedding[line[-2]]] = 1\n","    out_vec = w2v.get_vector(torch.FloatTensor(tmp_vector))\n","    out_vec = np.asarray(out_vec.detach().numpy(), dtype='float32')\n","    outvector_list.append(out_vec)\n","\n","    return invector_list, outvector_list\n","\n","def batchPad(batch):\n","    max_len = max([len(sent[0]) for sent in batch])\n","    if max_len <5:\n","        max_len = 5\n","    rows = []\n","    batch_labels = []\n","    for i in range(len(batch)):\n","        rows.append(np.pad(np.array(batch[i][0][:]), (0, max_len), 'constant', constant_values=1)[:max_len])\n","        batch_labels.append(batch[i][1])\n","        #print(batch_labels)\n","    return torch.LongTensor(np.concatenate(rows, axis=0).reshape(-1, max_len)), torch.LongTensor(batch_labels)\n","\n","def batchPad_1d(batch):\n","    max_len = 20\n","    rows = []\n","    batch_labels = []\n","    for i in range(len(batch)):\n","        rows.append(np.pad(np.array(batch[i][0][:]), (0, max_len), 'constant', constant_values=1)[:max_len])\n","        batch_labels.append(batch[i][1])\n","        #print(batch_labels)\n","    return torch.LongTensor(np.concatenate(rows, axis=0).reshape(-1, max_len)), torch.LongTensor(batch_labels)\n","\n","def weights_init(m):\n","    if type(m) == nn.Linear:\n","        nn.init.kaiming_uniform_(m.weight)\n","\n","def save_checkpoint(epoch, model, opt, path):\n","    state = {\n","        'Epoch': epoch,\n","        'State_dict': model.state_dict(),\n","        'optimizer': opt.state_dict()\n","    }\n","    torch.save(state, path)\n","\n","def MR_Preprocess(embed_kinds = 0):\n","    global w2v\n","    f_pos = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.pos\", 'r', encoding='latin-1')\n","    f_neg = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.neg\", 'r', encoding='latin-1')\n","    lines_pos = f_pos.readlines()\n","    lines_neg = f_neg.readlines()\n","    tokenized_pos = [strCleanup(line).split() for line in lines_pos]\n","    tokenized_neg = [strCleanup(line).split() for line in lines_neg]\n","    pos_label = [0] * len(tokenized_pos)\n","    neg_label = [1] * len(tokenized_neg)\n","    f_pos.close()\n","    f_neg.close()\n","    tokenized = tokenized_pos+tokenized_neg\n","    label = pos_label+neg_label\n","\n","    embedding_dic = {}\n","    #embedding_set = set()\n","    for line in tokenized:\n","        for word in line:\n","            try:\n","                tmp = embedding_dic[word]\n","            except KeyError:\n","                embedding_dic[word] = len(embedding_dic)\n","        #embedding_set.update(line)\n","    return tokenized, label, embedding_dic\n","\n","\n","def MR_Preprocess_CNN(embedder, embed_kinds = 0):\n","    global embedding\n","    global projectionlen\n","    global vocablen\n","    f_pos = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.pos\", 'r', encoding='latin-1')\n","    f_neg = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.neg\", 'r', encoding='latin-1')\n","    lines_pos = f_pos.readlines()\n","    lines_neg = f_neg.readlines()\n","    tokenized_pos = [strCleanup(line).split() for line in lines_pos]\n","    tokenized_neg = [strCleanup(line).split() for line in lines_neg]\n","    pos_label = [0] * len(tokenized_pos)\n","    neg_label = [1] * len(tokenized_neg)\n","    f_pos.close()\n","    f_neg.close()\n","    tokenized = tokenized_pos+tokenized_neg\n","    label = pos_label+neg_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    noword = 0\n","\n","    if not os.path.isfile(\"/content/drive/MyDrive/Data/rt-polaritydata/MR_dic_self.pickle\"):\n","        new_dic = {}\n","        new_arr = list()\n","        if embed_kinds == 2:\n","            tmp_dic = {}\n","            for word,val in embedding.items():\n","                tmp_vector = np.zeros(vocablen)\n","                tmp_vector[val] = 1\n","                vec = embedder.get_vector(torch.FloatTensor(tmp_vector))\n","                coefs = np.asarray(vec.detach().numpy(), dtype='float32')\n","                tmp_dic[word] = (coefs/np.linalg.norm(coefs))\n","            embedding_arr = np.zeros((len(embedding)+2, projectionlen), dtype=np.float32)\n","            rand = np.random.uniform(-1,1,size = projectionlen)\n","            embedding_arr[0] = rand/np.linalg.norm(rand)\n","            rand = np.random.uniform(-1,1,size = projectionlen)\n","            embedding_arr[1] = rand/np.linalg.norm(rand)\n","            for idx, (word_in, vec_in) in enumerate(tmp_dic.items()):\n","                embedding_arr[idx+2,:] = vec_in\n","                embedding_dic[word_in] = idx+2\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_dic_self.pickle', 'wb') as fw:\n","            pickle.dump(embedding_dic,fw)\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_arr_self.pickle', 'wb') as fa:\n","            pickle.dump(embedding_arr,fa)\n","    else:\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_dic_self.pickle', 'rb') as fw:\n","            embedding_dic = pickle.load(fw)\n","        with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_arr_self.pickle', 'rb') as fa:\n","            embedding_arr = pickle.load(fa)\n","    print(embedding_dic)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    tokenized_df = [[tokenized[i], label[i]] for i in range(len(tokenized))]\n","    print(embedding_arr.shape)\n","    return tokenized_df, tokenized_set, embedding_arr\n","\n","def MR_Process_CharWord2channel(word_embedder, char_embedder):\n","    global embedding\n","    global projectionlen\n","    global vocablen\n","    global alphabet_dic\n","    f_pos = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.pos\", 'r', encoding='latin-1')\n","    f_neg = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.neg\", 'r', encoding='latin-1')\n","    lines_pos = f_pos.readlines()\n","    lines_neg = f_neg.readlines()\n","    tokenized_pos = [strCleanup(line).split() for line in lines_pos]\n","    tokenized_neg = [strCleanup(line).split() for line in lines_neg]\n","    pos_label = [0] * len(tokenized_pos)\n","    neg_label = [1] * len(tokenized_neg)\n","    f_pos.close()\n","    f_neg.close()\n","    tokenized = tokenized_pos+tokenized_neg\n","    label = pos_label+neg_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    embedding_dic_char = {}\n","    noword = 0\n","\n","    embed_kinds = 2\n","\n","    if embed_kinds == 2:\n","        tmp_dic = {}\n","        tmp_dic_char = {}\n","        for word,val in embedding.items():\n","            tmp_vector = np.zeros(vocablen)\n","            tmp_vector[val] = 1\n","            vec = word_embedder.get_vector(torch.FloatTensor(tmp_vector))\n","            coefs = np.asarray(vec.detach().numpy(), dtype='float32')\n","            tmp_dic[word] = (coefs/np.linalg.norm(coefs))\n","            \n","            tmp_vector_char = np.zeros(len(alphabet_dic))\n","            for alphabet in word:\n","                try:\n","                    tmp_vector_char[alphabet_dic[alphabet]] = 1\n","                except KeyError:\n","                    continue\n","            vec_char = char_embedder.get_vector(torch.FloatTensor(tmp_vector_char))\n","            coefs_char = np.asarray(vec_char.detach().numpy(), dtype='float32')\n","            tmp_dic_char[word] = (coefs_char/np.linalg.norm(coefs_char))\n","\n","        embedding_arr = np.zeros((len(embedding)+2, projectionlen), dtype=np.float32)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_arr[0] = rand/np.linalg.norm(rand)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_arr[1] = rand/np.linalg.norm(rand)\n","        for idx, (word_in, vec_in) in enumerate(tmp_dic.items()):\n","            embedding_arr[idx+2,:] = vec_in\n","            embedding_dic[word_in] = idx+2\n","        \n","        embedding_char_arr = np.zeros((len(embedding)+2, projectionlen), dtype=np.float32)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_char_arr[0] = rand/np.linalg.norm(rand)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_char_arr[1] = rand/np.linalg.norm(rand)\n","        for idx, (word_in, vec_in) in enumerate(tmp_dic_char.items()):\n","            embedding_char_arr[idx+2,:] = vec_in\n","\n","    print(embedding_dic)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    tokenized_df = [[tokenized[i], label[i]] for i in range(len(tokenized))]\n","    print(embedding_arr.shape)\n","    return tokenized_df, tokenized_set, embedding_arr, embedding_char_arr\n","\n","def MR_Preprocess_GlobalVec(embed_kinds = 1):\n","    global WordVec\n","    f_pos = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.pos\", 'r', encoding='latin-1')\n","    f_neg = open(\"/content/drive/MyDrive/Data/rt-polaritydata/rt-polarity.neg\", 'r', encoding='latin-1')\n","    lines_pos = f_pos.readlines()\n","    lines_neg = f_neg.readlines()\n","    tokenized_pos = [strCleanup(line).split() for line in lines_pos]\n","    tokenized_neg = [strCleanup(line).split() for line in lines_neg]\n","    pos_label = [0] * len(tokenized_pos)\n","    neg_label = [1] * len(tokenized_neg)\n","    f_pos.close()\n","    f_neg.close()\n","    tokenized = tokenized_pos+tokenized_neg\n","    label = pos_label+neg_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    noword = 0\n","\n","    #if not os.path.isfile(\"/content/drive/MyDrive/Data/rt-polaritydata/MR_dic.pickle\"):\n","    if embed_kinds is 0:\n","        wv_0 = Word2Vec(tokenized, size=100, window=5, min_count=1)\n","        embedding_arr = np.zeros((len(wv_0.wv.vocab)+2, 100), dtype=np.float32)\n","        for idx, (word,vec) in enumerate(zip(wv_0.wv.vocab, wv_0.wv.vectors)):\n","            coefs = np.asarray(vec, dtype='float32')\n","            embedding_arr[idx+2,:] = (coefs/np.linalg.norm(coefs))\n","            embedding_dic[word] = idx+2\n","    elif embed_kinds is 1:\n","        tmp_dic = {}\n","        for word,vec in zip(w2v.vocab, w2v.vectors):\n","            if word in tokenized_set:\n","                tmp_dic[word] = vec\n","        for word in tokenized_set:\n","            if tmp_dic.get(word) is None:\n","                tmp_dic[word] = np.random.uniform(-1, 1, size=300)\n","                noword +=1\n","        embedding_arr = np.zeros((len(tmp_dic)+2, 300), dtype=np.float32)\n","        print(\"PRE words: \",len(tokenized_set) - noword)\n","        rand = np.random.uniform(-1,1,size = 300)\n","        embedding_arr[0] = rand/np.linalg.norm(rand)\n","        rand = np.random.uniform(-1,1,size = 300)\n","        embedding_arr[1] = rand/np.linalg.norm(rand)\n","        for idx, (word, vec) in enumerate(tmp_dic.items()):\n","            coefs = np.asarray(vec, dtype='float32')\n","            embedding_arr[idx+2,:] = (coefs/np.linalg.norm(coefs))\n","            embedding_dic[word] = idx+2\n","    #    with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_dic.pickle', 'wb') as fw:\n","    #        pickle.dump(embedding_dic,fw)\n","    #    with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_arr.pickle', 'wb') as fa:\n","    #        pickle.dump(embedding_arr,fa)\n","    #else:\n","    #    with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_dic.pickle', 'rb') as fw:\n","    #        embedding_dic = pickle.load(fw)\n","    #    with open('/content/drive/MyDrive/Data/rt-polaritydata/MR_arr.pickle', 'rb') as fa:\n","    #        embedding_arr = pickle.load(fa)\n","    print(embedding_dic)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    tokenized_df = [[tokenized[i], label[i]] for i in range(len(tokenized))]\n","    print(embedding_arr.shape)\n","    return tokenized_df, tokenized_set, embedding_arr\n","\n","def SUBJ_Preprocess(embed_kinds = 0):\n","    global w2v\n","    f_sub = open(\"/content/drive/MyDrive/Data/rotten_imdb.tar/quote.tok.gt9.5000\", 'r', encoding='latin-1')\n","    f_obj = open(\"/content/drive/MyDrive/Data/rotten_imdb.tar/plot.tok.gt9.5000\", 'r', encoding='latin-1')\n","    lines_sub = f_sub.readlines()\n","    lines_obj = f_obj.readlines()\n","    tokenized_sub = [strCleanup(line).split() for line in lines_sub]\n","    tokenized_obj = [strCleanup(line).split() for line in lines_obj]\n","    sub_label = [0] * len(tokenized_sub)\n","    obj_label = [1] * len(tokenized_obj)\n","    f_sub.close()\n","    f_obj.close()\n","    tokenized = tokenized_sub+tokenized_obj\n","    label = sub_label+obj_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    for line in tokenized:\n","        for word in line:\n","            try:\n","                tmp = embedding_dic[word]\n","            except KeyError:\n","                embedding_dic[word] = len(embedding_dic)\n","        #embedding_set.update(line)\n","    return tokenized, label, embedding_dic\n","\n","def SUBJ_Preprocess_CharWord2channel(word_embedder, char_embedder):\n","    global w2v\n","    f_sub = open(\"/content/drive/MyDrive/Data/rotten_imdb.tar/quote.tok.gt9.5000\", 'r', encoding='latin-1')\n","    f_obj = open(\"/content/drive/MyDrive/Data/rotten_imdb.tar/plot.tok.gt9.5000\", 'r', encoding='latin-1')\n","    lines_sub = f_sub.readlines()\n","    lines_obj = f_obj.readlines()\n","    tokenized_sub = [strCleanup(line).split() for line in lines_sub]\n","    tokenized_obj = [strCleanup(line).split() for line in lines_obj]\n","    sub_label = [0] * len(tokenized_sub)\n","    obj_label = [1] * len(tokenized_obj)\n","    f_sub.close()\n","    f_obj.close()\n","    tokenized = tokenized_sub+tokenized_obj\n","    label = sub_label+obj_label\n","    tokenized_set = []\n","    for sent in tokenized:\n","        tokenized_set.extend(sent)\n","    tokenized_set = list(set(tokenized_set))\n","    embedding_dic = {}\n","    embedding_dic_char = {}\n","    noword = 0\n","\n","    embed_kinds = 2\n","\n","    if embed_kinds == 2:\n","        tmp_dic = {}\n","        tmp_dic_char = {}\n","        for word,val in embedding.items():\n","            tmp_vector = np.zeros(vocablen)\n","            tmp_vector[val] = 1\n","            vec = word_embedder.get_vector(torch.FloatTensor(tmp_vector))\n","            coefs = np.asarray(vec.detach().numpy(), dtype='float32')\n","            tmp_dic[word] = (coefs/np.linalg.norm(coefs))\n","            \n","            tmp_vector_char = np.zeros(len(alphabet_dic))\n","            for alphabet in word:\n","                try:\n","                    tmp_vector_char[alphabet_dic[alphabet]] = 1\n","                except KeyError:\n","                    continue\n","            vec_char = char_embedder.get_vector(torch.FloatTensor(tmp_vector_char))\n","            coefs_char = np.asarray(vec_char.detach().numpy(), dtype='float32')\n","            tmp_dic_char[word] = (coefs_char/np.linalg.norm(coefs_char))\n","\n","        embedding_arr = np.zeros((len(embedding)+2, projectionlen), dtype=np.float32)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_arr[0] = rand/np.linalg.norm(rand)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_arr[1] = rand/np.linalg.norm(rand)\n","        for idx, (word_in, vec_in) in enumerate(tmp_dic.items()):\n","            embedding_arr[idx+2,:] = vec_in\n","            embedding_dic[word_in] = idx+2\n","        \n","        embedding_char_arr = np.zeros((len(embedding)+2, projectionlen), dtype=np.float32)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_char_arr[0] = rand/np.linalg.norm(rand)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_char_arr[1] = rand/np.linalg.norm(rand)\n","        for idx, (word_in, vec_in) in enumerate(tmp_dic_char.items()):\n","            embedding_char_arr[idx+2,:] = vec_in\n","\n","    print(embedding_dic)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    tokenized_df = [[tokenized[i], label[i]] for i in range(len(tokenized))]\n","    print(embedding_arr.shape)\n","    return tokenized_df, tokenized_set, embedding_arr, embedding_char_arr\n","\n","def TREC_Preprocess_CharWord2channel(word_embedder, char_embedder):\n","    f_train = open(\"/content/drive/MyDrive/Data/TREC/train_5500.label.txt\", 'r', encoding='latin-1')\n","    f_test = open(\"/content/drive/MyDrive/Data/TREC/TREC_10.label.txt\", 'r', encoding='latin-1')\n","    lines_train = f_train.readlines()\n","    lines_test = f_test.readlines()\n","    train_label = []\n","    test_label = []\n","    train_tokenized = []\n","    test_tokenized = []\n","\n","    for line in lines_train:\n","        line = line.split(\":\")\n","        train_label.append(line[0])\n","        # ? is in every sentence so delete\n","        line = strCleanup(line[1]).split()[1:-1]\n","        train_tokenized.append(line)\n","    for line in lines_test:\n","        line = line.split(\":\")\n","        test_label.append(line[0])\n","        # ? is in every sentence so delete\n","        line = strCleanup(line[1]).split()[1:-1]\n","        test_tokenized.append(line)\n","    tokenized = train_tokenized+test_tokenized\n","    f_train.close()\n","    f_test.close()\n","    def make_label(cat):\n","        if cat == 'ABBR':\n","            return 0\n","        elif cat == 'ENTY':\n","            return 1\n","        elif cat == 'DESC':\n","            return 2\n","        elif cat == 'HUM':\n","            return 3\n","        elif cat == 'LOC':\n","            return 4\n","        elif cat == 'NUM':\n","            return 5\n","    label = np.array(train_label+test_label)\n","    lebelling = np.vectorize(make_label)\n","    label = lebelling(label)\n","    tokenized_set = []\n","    embedding_dic_char = {}\n","    embedding_dic = {}\n","    noword = 0\n","\n","    embed_kinds = 2\n","\n","    if embed_kinds == 2:\n","        tmp_dic = {}\n","        tmp_dic_char = {}\n","        for word,val in embedding.items():\n","            tmp_vector = np.zeros(vocablen)\n","            tmp_vector[val] = 1\n","            vec = word_embedder.get_vector(torch.FloatTensor(tmp_vector))\n","            coefs = np.asarray(vec.detach().numpy(), dtype='float32')\n","            tmp_dic[word] = (coefs/np.linalg.norm(coefs))\n","            \n","            tmp_vector_char = np.zeros(len(alphabet_dic))\n","            for alphabet in word:\n","                try:\n","                    tmp_vector_char[alphabet_dic[alphabet]] = 1\n","                except KeyError:\n","                    continue\n","            vec_char = char_embedder.get_vector(torch.FloatTensor(tmp_vector_char))\n","            coefs_char = np.asarray(vec_char.detach().numpy(), dtype='float32')\n","            tmp_dic_char[word] = (coefs_char/np.linalg.norm(coefs_char))\n","\n","        embedding_arr = np.zeros((len(embedding)+2, projectionlen), dtype=np.float32)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_arr[0] = rand/np.linalg.norm(rand)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_arr[1] = rand/np.linalg.norm(rand)\n","        for idx, (word_in, vec_in) in enumerate(tmp_dic.items()):\n","            embedding_arr[idx+2,:] = vec_in\n","            embedding_dic[word_in] = idx+2\n","        \n","        embedding_char_arr = np.zeros((len(embedding)+2, projectionlen), dtype=np.float32)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_char_arr[0] = rand/np.linalg.norm(rand)\n","        rand = np.random.uniform(-1,1,size = projectionlen)\n","        embedding_char_arr[1] = rand/np.linalg.norm(rand)\n","        for idx, (word_in, vec_in) in enumerate(tmp_dic_char.items()):\n","            embedding_char_arr[idx+2,:] = vec_in\n","\n","    print(embedding_dic)\n","    makeIdx(tokenized, embedding_dic)\n","    print(len(tokenized))\n","    print(len(tokenized[0]))\n","    train_df = list()\n","    test_df = list()\n","    for idx in range(len(tokenized)):\n","        if idx < len(train_tokenized):\n","            train_df.append([tokenized[idx], label[idx]])\n","        else:\n","            test_df.append([tokenized[idx], label[idx]])\n","    return train_df, test_df, tokenized_set, embedding_arr, embedding_char_arr\n","\n","def TREC_Preprocess(embed_kinds = 0):\n","    f_train = open(\"/content/drive/MyDrive/Data/TREC/train_5500.label.txt\", 'r', encoding='latin-1')\n","    f_test = open(\"/content/drive/MyDrive/Data/TREC/TREC_10.label.txt\", 'r', encoding='latin-1')\n","    lines_train = f_train.readlines()\n","    lines_test = f_test.readlines()\n","    train_label = []\n","    test_label = []\n","    train_tokenized = []\n","    test_tokenized = []\n","\n","    for line in lines_train:\n","        line = line.split(\":\")\n","        train_label.append(line[0])\n","        # ? is in every sentence so delete\n","        line = strCleanup(line[1]).split()[1:-1]\n","        train_tokenized.append(line)\n","    for line in lines_test:\n","        line = line.split(\":\")\n","        test_label.append(line[0])\n","        # ? is in every sentence so delete\n","        line = strCleanup(line[1]).split()[1:-1]\n","        test_tokenized.append(line)\n","    tokenized = train_tokenized+test_tokenized\n","    f_train.close()\n","    f_test.close()\n","    def make_label(cat):\n","        if cat == 'ABBR':\n","            return 0\n","        elif cat == 'ENTY':\n","            return 1\n","        elif cat == 'DESC':\n","            return 2\n","        elif cat == 'HUM':\n","            return 3\n","        elif cat == 'LOC':\n","            return 4\n","        elif cat == 'NUM':\n","            return 5\n","    label = np.array(train_label+test_label)\n","    lebelling = np.vectorize(make_label)\n","    label = lebelling(label)\n","    tokenized_set = []\n","    embedding_dic = {}\n","    for line in tokenized:\n","        for word in line:\n","            try:\n","                tmp = embedding_dic[word]\n","            except KeyError:\n","                embedding_dic[word] = len(embedding_dic)\n","    return tokenized, label, embedding_dic\n","\n","MR_tokenized, MR_label, embedding = MR_Preprocess(0)\n","vocablen = len(embedding)\n","\n","print(vocablen)\n","projectionlen = 100\n","window_size = 2 #앞 뒤로 2개씩\n","#print(len(AG_tokenized))\n","print(len(embedding))\n","\n","w2v = W2V(vocablen, projectionlen, 0.5)\n","checkpoint_word = torch.load(\"/content/drive/MyDrive/model/w2v_MR.pt\")\n","w2v.load_state_dict(checkpoint_word['State_dict'])\n","dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('current device', dev)\n","num_epochs = 10\n","batch_size = 64\n","running_loss = 0\n","correct = 0\n","total = 0\n","\n","loss_func = nn.CrossEntropyLoss()\n","loss_func_reg = nn.MSELoss()\n","#parameters = filter(lambda p: p.requires_grad, w2v.parameters())\n","#w2v.apply(weights_init)\n","#w2v.to(dev)\n","checkpoint = torch.load(\"/content/drive/MyDrive/model/char2v(chartochar)_MR.pt\")\n","#char2v = Char2WV(len(alphabet_dic), vocablen, projectionlen, 0.5)\n","char2v = W2V(len(alphabet_dic), projectionlen, 0.5)\n","parameters = filter(lambda p: p.requires_grad, char2v.parameters())\n","#char2v.apply(weights_init)\n","char2v.load_state_dict(checkpoint['State_dict'])\n","#char2v.to(dev)\n","#tokenized_df, tokenized_set, lookup_table = MR_Preprocess_CNN(char2v, 2)\n","#opt = optim.SGD(parameters, lr = 1e-2, momentum = 0.9)\n","\n","tokenized_df, tokenized_set, lookup_table_word, lookup_table_char = MR_Process_CharWord2channel(w2v, char2v)\n","#tokenized_df, tokenized_set, lookup_table_word, lookup_table_char = SUBJ_Preprocess_CharWord2channel(w2v, char2v)\n","#tokenized_Train, tokenized_Test, tokenized_set, lookup_table_word, lookup_table_char = TREC_Preprocess_CharWord2channel(w2v, char2v)\n","#tokenized_Train = tokenized_df[:4500]+tokenized_df[5000:9500]\n","#tokenized_Test = tokenized_df[4500:5000]+tokenized_df[9500:]\n","#parameters = filter(lambda p: p.requires_grad, char2v.parameters())\n","#char2v.apply(weights_init)\n","\n","opt = optim.SGD(parameters, lr = 1e-2, momentum = 0.9)\n","\n","running_loss = 0\n","correct = 0\n","total = 0\n","#data_addr = \"/content/drive/MyDrive/Data_CharacterConvNet\"\n","#WordVec = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin', binary=True)\n","\n","#save_checkpoint(num_epochs, w2v, optimizer,\"/content/drive/MyDrive/model/w2v_MR.pt\")\n","#checkpoint = torch.load(\"/content/drive/MyDrive/model/w2v_MR.pt\", map_location=torch.device('cpu'))\n","#w2v.load_state_dict(checkpoint['State_dict'])\n","#opt.load_state_dict(checkpoint['optimizer'])\n","#tokenized_df, tokenized_set, lookup_table = MR_Preprocess_CNN(w2v, 2)\n","tokenized_Train = tokenized_df[:5000]+tokenized_df[5331:10331]\n","tokenized_Test = tokenized_df[5000:5331]+tokenized_df[10331:]\n","\n","\n","net = Net_Deep(lookup_table_word, lookup_table_char, [3,4,5],100, 2, 0.5, mode='multi-channel')\n","\n","# HyperParameters\n","\n","net.to(dev)\n","num_epochs = 100\n","batch_size = 64\n","param = filter(lambda p: p.requires_grad, net.parameters())\n","optimizer = optim.Adadelta(param)\n","\n","torch.manual_seed(99)\n","running_loss = 0\n","correct = 0\n","total = 0\n","\n","trainloader = torch.utils.data.DataLoader(tokenized_Train, batch_size=batch_size, shuffle=True, collate_fn=batchPad)\n","testloader = torch.utils.data.DataLoader(tokenized_Test, batch_size=batch_size, shuffle= True, collate_fn=batchPad)\n","\n","start = time.time()\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    for idx, (x, y) in enumerate(iter(trainloader), 0):\n","        net.train()\n","        x, y = x.to(dev), y.to(dev)\n","\n","        out = net(x)\n","        loss = loss_func(out, y)\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(param, max_norm=3)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        running_loss += loss.item()\n","        if idx % 50 == 49:\n","            correct = (torch.max(out, 1)[1] == y).sum().item()\n","            total = batch_size\n","            print('Training Accuracy: %d ' % (100.0 * correct / total))\n","            print('%d/%d' % (correct, total))\n","            print('RunningLoss %5d: %.3f' % (idx + 1, running_loss))\n","\n","print(\"training time :\", time.time() - start)\n","correct_eachbatch = 0\n","total_eachbatch = 0\n","correct = 0\n","total = 0\n","net.eval()\n","with torch.no_grad():\n","    for (x, y) in iter(testloader):\n","        x, y = x.to(dev), y.to(dev)\n","        out = net.forward(x)\n","        predicted = torch.max(out, 1)[1]\n","        loss = loss_func(out, y)\n","        print(\"test loss: \", loss.item())\n","        correct_eachbatch += (predicted == y).sum().item()\n","        correct += correct_eachbatch\n","        total_eachbatch += len(y)\n","        total += total_eachbatch\n","        correct_eachbatch = 0\n","        total_eachbatch = 0\n","        print('Accuracy for test_batch: %.3f %%' % (100.0 * correct / total))\n","        print('--------------------------------')\n","\n","print('Accuracy for test: %.3f %%' % (100.0 * correct / total))\n","print('--------------------------------')\n","\"\"\"\n","\n","for epoch in range(num_epochs):\n","    for idx, line in enumerate(MR_tokenized):\n","        if len(line) < 3:\n","            continue\n","        in_list,out_list = batch_onehot_window1_chartoword(line)\n","\n","        char2v.train()\n","        in_list, out_list = torch.FloatTensor(in_list), torch.from_numpy(np.array(out_list)).float()\n","        in_list, out_list = in_list.to(dev), out_list.to(dev)\n","\n","        out = char2v(in_list)\n","        loss = loss_func_reg(out, out_list)\n","        loss.backward()\n","        opt.step()\n","        opt.zero_grad()\n","        running_loss += loss.item()\n","        if idx%100 == 0:\n","            print(idx)\n","    print('RunningLoss %5d: %.3f' %(epoch, running_loss))\n","    running_loss = 0\n","save_checkpoint(num_epochs, char2v, opt,\"/content/drive/MyDrive/model/char2v(chartoword)_MR.pt\")\n","\"\"\"\n"],"execution_count":null,"outputs":[]}]}
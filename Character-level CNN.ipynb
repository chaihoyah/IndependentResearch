{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Character-level CNN.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1wObruEkSsM3AFZOFDaurXRexe4_LUgPN","authorship_tag":"ABX9TyOYKF9aD/SnzPAN1H34HdYb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"_enzbqEWDSQx"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import re\n","\n","class Net(nn.Module):\n","    def __init__(self, input_feature, input_length, num_classes, drop_prob, mode='small'):\n","        super(Net, self).__init__()\n","        np.random.seed(99)\n","        self.mode = mode\n","        if self.mode == 'small':\n","            l6_frame_length = int((input_length - 96)/27)\n","            self.conv = nn.Sequential(\n","                nn.Conv1d(input_feature, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3)\n","            )\n","\n","            self.fc = nn.Sequential(\n","                nn.Linear(l6_frame_length * 256, 1024),\n","                nn.Dropout(p=drop_prob),\n","                nn.Linear(1024, 1024),\n","                nn.Dropout(p=drop_prob),\n","                nn.Linear(1024, num_classes)\n","            )\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = out.view(len(x), -1)\n","        out = self.fc(out)\n","        return self.softmax(out)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6yTZppkJewf"},"source":["import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from nltk.corpus import wordnet\n","from collections import OrderedDict\n","import nltk\n","import pickle\n","import os\n","import copy\n","import re\n","\n","data_addr = \"/content/drive/MyDrive/Data_CharacterConvNet\"\n","nltk.download('wordnet')\n","\n","alphabet_dic = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14, 'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25,\n"," '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35,\n"," \"-\":36, ',':37, ';':38, '.':39, '!':40, '?':41, ':':42, '\\'':43, '\"':44, '/':45, '\\\\':46, '|':47, '_':48, '@':49, '#':50, '$':51, '%':52, 'Ë†':53, '&':54, '*':55, '~':56, '`':57, '+':58, \"-\":59, '=':60, '<':61, '>':62, '(':63, ')':64, '[':65, ']':66, '{':67, '}':68, '\\n': 69}\n","\n","alphabet_length = len(alphabet_dic)\n","print(alphabet_length)\n","char_length = 1014\n","\n","def get_data(path, col_num, augment_data, doLower):\n","    global data_addr\n","    global alphabet_dic\n","    global alphabet_length\n","    global char_length\n","\n","    def str_lower(str):\n","        return str.lower()\n","\n","    def to_str(num):\n","        return str(num)\n","\n","    def label_process(label):\n","        return label-1\n","\n","    full_addr = data_addr + path\n","    if not os.path.isfile(data_addr+path+'/pickles'+'train_basic.pickle'):\n","        if col_num == 3:\n","            train_csv = pd.read_csv(full_addr+'/train.csv', names=['label', 'title', 'data'])\n","            test_csv = pd.read_csv(full_addr+'/test.csv', names=['label', 'title', 'data'])\n","            train_csv['title'] = train_csv['title'].transform(to_str)\n","            test_csv['title'] = test_csv['title'].transform(to_str)\n","            train_csv['full data'] = train_csv[['title', 'data']].apply(''.join, axis=1)\n","            test_csv['full data'] = test_csv[['title', 'data']].apply(''.join, axis=1)\n","            if doLower:\n","                train_csv['full data'] = train_csv['full data'].transform(str_lower)\n","                test_csv['full data'] = test_csv['full data'].transform(str_lower)\n","            train_csv['label'] = train_csv['label'].transform(label_process)\n","            test_csv['label'] = test_csv['label'].transform(label_process)\n","            train_df = [[row['full data'], row['label']] for index,row in train_csv.iterrows()]\n","            test_df = [[row['full data'], row['label']] for index,row in test_csv.iterrows()]\n","        elif col_num == 4:\n","            train_csv = pd.read_csv(full_addr+'/train.csv', names=['label', 'title', 'question', 'answer'])\n","            test_csv = pd.read_csv(full_addr+'/test.csv', names=['label', 'title', 'question', 'answer'])\n","            print(test_csv['question'].head())\n","            print(test_csv['title'].head())\n","            print(test_csv['answer'].head())\n","            train_csv['title'] = train_csv['title'].transform(to_str)\n","            print(len(train_csv['answer']))\n","            train_csv['question'] = train_csv['question'].transform(to_str)\n","            train_csv['answer'] = train_csv['answer'].transform(to_str)\n","            test_csv['title'] = test_csv['title'].transform(to_str)\n","            test_csv['question'] = test_csv['question'].transform(to_str)\n","            test_csv['answer'] = test_csv['answer'].transform(to_str)\n","            train_csv['full data'] = train_csv[['title', 'question', 'answer']].apply(' '.join, axis=1)\n","            test_csv['full data'] = test_csv[['title', 'question', 'answer']].apply(' '.join, axis=1)\n","            print(train_csv['full data'])\n","            if doLower:\n","                train_csv['full data'] = train_csv['full data'].transform(str_lower)\n","                test_csv['full data'] = test_csv['full data'].transform(str_lower)\n","            train_csv['label'] = train_csv['label'].transform(label_process)\n","            test_csv['label'] = test_csv['label'].transform(label_process)\n","            print(train_csv['full data'])\n","            train_df = [[row['full data'], row['label']] for index,row in train_csv.iterrows()]\n","            test_df = [[row['full data'], row['label']] for index,row in test_csv.iterrows()]\n","            print(len(train_df))\n","        elif col_num == 2:\n","            train_csv = pd.read_csv(full_addr + '/train.csv', names=['label', 'data'])\n","            test_csv = pd.read_csv(full_addr + '/test.csv', names=['label', 'data'])\n","            if doLower:\n","                train_csv['data'] = train_csv['data'].transform(str_lower)\n","                test_csv['data'] = test_csv['data'].transform(str_lower)\n","            print(test_csv['data'].head())\n","            print(train_csv['label'])\n","            train_csv['label'] = train_csv['label'].transform(label_process)\n","            test_csv['label'] = test_csv['label'].transform(label_process)\n","            train_df = [[row['data'], row['label']] for index, row in train_csv.iterrows()]\n","            test_df = [[row['data'], row['label']] for index, row in test_csv.iterrows()]  \n","        np.random.shuffle(train_df)\n","        np.random.shuffle(test_df)\n","        with open(data_addr+path+'/pickles'+'train_basic.pickle', 'wb') as fw:\n","            pickle.dump(train_df, fw)\n","        with open(data_addr+path+'/pickles'+'test_basic.pickle', 'wb') as fa:\n","            pickle.dump(test_df, fa)\n","        \n","    else:\n","        with open(data_addr+path+'/pickles'+'train_basic.pickle', 'rb') as fw:\n","            train_df = pickle.load(fw)\n","        with open(data_addr+path+'/pickles'+'test_basic.pickle', 'rb') as fa:\n","            test_df = pickle.load(fa)\n","\n","    def str_cleanup(str):\n","        return str.strip().split()\n","\n","    def find_synonyms(word):\n","        syn = list()\n","        for synset in wordnet.synsets(word):\n","            for syn_word in synset.lemma_names():\n","                syn.append(syn_word)\n","        return list(OrderedDict.fromkeys(syn))\n","\n","    if augment_data == True:\n","        if not os.path.isfile(data_addr + path + '/pickles' + 'train_augmented.pickle'):\n","            vocab_set = set()\n","            train_augmented_df = copy.deepcopy(train_df)\n","            for idx_out, (data, label) in enumerate(train_df):\n","                data = str_cleanup(data)\n","                syn_list = list()\n","                replaceable_len = 0\n","                for idx_in,word in enumerate(data):\n","                    tmp = find_synonyms(word)\n","                    if len(tmp) > 0:\n","                        replaceable_len += 1\n","                        syn_list.append([idx_in, tmp])\n","                replace_num = np.random.geometric(p=0.5)\n","                if replaceable_len >0 and replace_num > 0:\n","                    replace_list = np.random.choice(replaceable_len, replace_num)\n","                    data = np.array(data)\n","                    for num in replace_list:\n","                        replace_word_len = len(syn_list[num][1])\n","                        replace_word_num = np.random.geometric(p=0.5)\n","                        if len(syn_list[num][1]) > replace_word_len:\n","                            data[syn_list[num][0]] = syn_list[num][1][replace_word_num]\n","                        else:\n","                            data[syn_list[num][0]] = syn_list[num][1][-1]\n","                    train_augmented_df.append([' '.join(data), label])\n","            with open(data_addr+path+'/pickles'+'train_augmented.pickle', 'wb') as fw:\n","                pickle.dump(train_augmented_df, fw)\n","        else:\n","            with open(data_addr+path+'/pickles'+'train_augmented.pickle', 'rb') as fw:\n","                train_augmented_df = pickle.load(fw)\n","        return train_augmented_df, test_df\n","    return train_df, test_df\n","\n","\n","# Press the green button in the gutter to run the script.\n","def onehot_encode(batch):\n","    out = torch.zeros(len(batch), alphabet_length, char_length) # (128, 69, 1014)?\n","    out_label = []\n","    for idx,item in enumerate(batch):\n","        out_label.append(item[1])\n","        for idx_in, char in enumerate(item[0][:-1015:-1]):\n","            try:\n","                out[idx][alphabet_dic[char]][idx_in] = 1\n","            except KeyError:\n","                continue\n","    return torch.Tensor(out), torch.LongTensor(out_label)\n","\n","def weights_init(m):\n","    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, mean=0, std=0.05)\n","\n","def save_checkpoint(epoch, model, opt, path):\n","    state = {\n","        'Epoch': epoch,\n","        'State_dict': model.state_dict(),\n","        'optimizer': opt.state_dict()\n","    }\n","    torch.save(state, path)\n","\n","# ag, amazon, dbpedia, sogou -> 3 cols, label, title, data\n","# yahoo -> 4 cols, label, title, question, answer\n","# yelp -> 2 cols, label, data\n","# ag\n","net = Net(alphabet_length, char_length, 10, 0.5, 'small')\n","net.apply(weights_init)\n","dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('current device: ', dev)\n","net.to(dev)\n","\n","num_epochs = 21\n","batch_size = 128\n","loss_func = nn.CrossEntropyLoss()\n","lr = 1e-2\n","optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","torch.manual_seed(99)\n","running_loss = 0\n","correct = 0\n","total = 0\n","\n","if __name__ == '__main__':\n","    train_data, test_data = get_data(\"/yahoo_answers_csv\", 4, False, True)\n","    one = int(len(train_data)/2)\n","    trainloader_list = []\n","    trainloader_list.append(torch.utils.data.DataLoader(train_data[:one], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    trainloader_list.append(torch.utils.data.DataLoader(train_data[one:], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","\n","    testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn = onehot_encode)\n","\n","    for epoch in range(num_epochs):\n","        if epoch != 0 and epoch%3 == 0:\n","            lr *= 0.5\n","            optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","\n","        running_loss = 0\n","        correct = 0\n","        total = 0\n","        net.train()\n","        trainloader = trainloader_list[epoch % 2]\n","\n","        for idx, (x, y) in enumerate(iter(trainloader), 0):\n","            x, y = x.to(dev), y.to(dev)\n","            out = net(x)\n","            loss = loss_func(out, y)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            running_loss += loss.item()\n","            if idx % 50 == 49:\n","                correct = (torch.max(out, 1)[1] == y).sum().item()\n","                total = batch_size\n","                print('Training Accuracy: %d ' % (100.0 * correct / total))\n","                print('%d/%d' % (correct, total))\n","                print('RunningLoss %5d: %.3f' % (idx + 1, running_loss))\n","                running_loss = 0\n","\n","        correct_eachbatch = 0\n","        total_eachbatch = 0\n","        correct = 0\n","        total = 0\n","        net.eval()\n","        if epoch == 10 or epoch == 20 or epoch == 31:\n","            print(\"epoch: \", epoch)\n","            with torch.no_grad():\n","                for (x, y) in iter(testloader):\n","                    x, y = x.to(dev), y.to(dev)\n","                    out = net.forward(x)\n","                    predicted = torch.max(out, 1)[1]\n","                    loss = loss_func(out, y)\n","                    print(\"test loss: \", loss.item())\n","                    correct_eachbatch += (predicted == y).sum().item()\n","                    correct += correct_eachbatch\n","                    print(len(y))\n","                    total_eachbatch += len(y)\n","                    print(total_eachbatch)\n","                    total += total_eachbatch\n","                    print('Accuracy for test_batch: %.3f %%' % (100.0 * correct_eachbatch / total_eachbatch))\n","                    print('--------------------------------')\n","                    correct_eachbatch = 0\n","                    total_eachbatch = 0\n","            print('Accuracy for test: %.3f %%' % (100.0 * correct / total))\n","            print('--------------------------------')\n","    correct_eachbatch = 0\n","    total_eachbatch = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for (x, y) in iter(testloader):\n","            x, y = x.to(dev), y.to(dev)\n","            out = net.forward(x)\n","            predicted = torch.max(out, 1)[1]\n","            loss = loss_func(out, y)\n","            print(\"test loss: \", loss.item())\n","            correct_eachbatch += (predicted == y).sum().item()\n","            correct += correct_eachbatch\n","            print(len(y))\n","            total_eachbatch += len(y)\n","            print(total_eachbatch)\n","            total += total_eachbatch\n","            print('Accuracy for test_batch: %.3f %%' % (100.0 * correct_eachbatch / total_eachbatch))\n","            print('--------------------------------')\n","            correct_eachbatch = 0\n","            total_eachbatch = 0\n","        \n","    print('Accuracy for test: %.3f %%' % (100.0 * correct / total))\n","    print('--------------------------------')\n","    save_checkpoint(num_epochs, net, optimizer, data_addr+'/model'+ '/yahoo_answers_basic_21epoch')\n"],"execution_count":null,"outputs":[]}]}